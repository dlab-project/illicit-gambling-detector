import time
import json
import requests
import os
from typing import Dict, Any
from urllib.parse import urlparse
from dotenv import load_dotenv

from .keyword_manager import KeywordManager
from .search_engine import SearchEngine
from .url_extractor import URLExtractor
from .json_storage import JSONStorage
from .gemini_classifier import GeminiClassifier


class GamblingDomainCrawler:
    def __init__(self, settings_file: str = "settings.json"):
        # .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
        load_dotenv()
        
        self.settings = self._load_settings(settings_file)
        self.keyword_manager = KeywordManager()
        self.search_engine = SearchEngine(headless=self.settings.get("headless_mode", True))
        self.url_extractor = URLExtractor(
            remove_tracking_params=self.settings.get("remove_tracking_params", True)
        )
        self.storage = JSONStorage(self.settings.get("output_file", "results.json"))
        
        # Gemini ë¶„ë¥˜ê¸° ì´ˆê¸°í™” (.env íŒŒì¼ì—ì„œ API í‚¤ ìë™ ë¡œë“œ)
        try:
            # GeminiClassifierëŠ” ìë™ìœ¼ë¡œ .envì—ì„œ GEMINI_API_KEYë¥¼ ë¡œë“œí•¨
            self.classifier = GeminiClassifier()
            self.use_classifier = self.settings.get("use_gemini_classifier", True)
        except ValueError as e:
            print(f"âš ï¸ ê²½ê³ : Gemini ë¶„ë¥˜ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨ - {e}")
            self.classifier = None
            self.use_classifier = False

    def _load_settings(self, settings_file: str) -> Dict[str, Any]:
        # ì„¤ì • íŒŒì¼ ë¡œë“œ
        with open(settings_file, 'r', encoding='utf-8') as f:
            return json.load(f)

    def _fetch_html_from_url(self, url: str, timeout: int = 10) -> str:
        """URLì—ì„œ HTML ì½˜í…ì¸ ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, headers=headers, timeout=timeout)
            response.encoding = response.apparent_encoding or 'utf-8'
            return response.text
        except Exception as e:
            print(f"  âŒ URL ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ {url}: {e}")
            return ""

    def _classify_visited_results(self, visited_results: list) -> tuple:
        """
        ë°©ë¬¸í•œ ê²°ê³¼(URLê³¼ HTML ìŒ)ë¥¼ ë¶„ë¥˜í•˜ê³  ë¶ˆë²• ë„ë°• ì‚¬ì´íŠ¸ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤
        
        Args:
            visited_results: [(url, html_content), ...] í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            (filtered_urls, classification_results) íŠœí”Œ
        """
        if not self.use_classifier or not self.classifier:
            # ë¶„ë¥˜ê¸°ê°€ ì—†ìœ¼ë©´ ëª¨ë“  URL ë°˜í™˜
            return [url for url, _ in visited_results], None

        filtered_urls = []
        classification_results = []

        for url, html_content in visited_results:
            if not html_content:
                print(f"  â­ï¸ ê±´ë„ˆë›°ê¸° {url} - HTML ì½˜í…ì¸  ì—†ìŒ")
                continue

            # Geminië¡œ ë¶ˆë²• ì‚¬ì´íŠ¸ íŒë³„
            print(f"  ğŸ” ë¶„ë¥˜ ì¤‘: {url}")
            result = self.classifier.classify_url(url, html_content)
            classification_results.append(result)

            # ì˜¤ë¥˜ê°€ ì—†ê³  ë¶ˆë²• ì‚¬ì´íŠ¸ë©´ í•„í„°ë§ëœ ëª©ë¡ì— ì¶”ê°€
            if result.get("error") is None and result.get("is_illegal"):
                print(f"  âœ… ë¶ˆë²• ì‚¬ì´íŠ¸ íƒì§€: {url} (ì‹ ë¢°ë„: {result.get('confidence', 0):.2f})")
                filtered_urls.append(url)
            elif result.get("error"):
                print(f"  âš ï¸ ë¶„ë¥˜ ì˜¤ë¥˜ {url}: {result.get('error')}")
            else:
                print(f"  âŒ ë¶ˆë²• ë„ë°• ì‚¬ì´íŠ¸ ì•„ë‹˜: {url}")

        return filtered_urls, classification_results

    def _classify_and_filter_urls(self, urls: list) -> tuple:
        """
        URL ëª©ë¡ì„ ë¶„ë¥˜í•˜ê³  ë¶ˆë²• ë„ë°• ì‚¬ì´íŠ¸ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤
        
        Returns:
            (filtered_urls, classification_results) íŠœí”Œ
        """
        if not self.use_classifier or not self.classifier:
            return urls, None

        filtered_urls = []
        classification_results = []

        for url in urls:
            # URLì—ì„œ HTML ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸°
            print(f"  ğŸ“¥ ì½˜í…ì¸  ê°€ì ¸ì˜¤ëŠ” ì¤‘: {url}")
            html_content = self._fetch_html_from_url(url)

            if not html_content:
                # HTMLì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
                print(f"  â­ï¸ ê±´ë„ˆë›°ê¸° {url} - ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨")
                continue

            # Geminië¡œ ë¶ˆë²• ì‚¬ì´íŠ¸ íŒë³„
            print(f"  ğŸ” ë¶„ë¥˜ ì¤‘: {url}")
            result = self.classifier.classify_url(url, html_content)
            classification_results.append(result)

            # ì˜¤ë¥˜ê°€ ì—†ê³  ë¶ˆë²• ì‚¬ì´íŠ¸ë©´ í•„í„°ë§ëœ ëª©ë¡ì— ì¶”ê°€
            if result.get("error") is None and result.get("is_illegal"):
                print(f"  âœ… ë¶ˆë²• ì‚¬ì´íŠ¸ íƒì§€: {url} (ì‹ ë¢°ë„: {result.get('confidence', 0):.2f})")
                filtered_urls.append(url)
            elif result.get("error"):
                print(f"  âš ï¸ ë¶„ë¥˜ ì˜¤ë¥˜ {url}: {result.get('error')}")
            else:
                print(f"  âŒ ë¶ˆë²• ë„ë°• ì‚¬ì´íŠ¸ ì•„ë‹˜: {url}")

        return filtered_urls, classification_results

    def crawl(self):
        print("ğŸš€ ë¶ˆë²• ë„ë°• ì‚¬ì´íŠ¸ í¬ë¡¤ëŸ¬ ì‹œì‘...")

        # í‚¤ì›Œë“œ ì¡°í•© ìƒì„±
        keywords = self.keyword_manager.generate_combinations()
        print(f"ğŸ“‹ {len(keywords)}ê°œì˜ í‚¤ì›Œë“œ ì¡°í•© ìƒì„± ì™„ë£Œ")

        delay = self.settings.get("delay_between_searches", 2)
        max_links_per_search = self.settings.get("max_links_per_search", 10)
        existing_urls = self.storage.get_existing_urls()

        # ëª¨ë“  í‚¤ì›Œë“œì— ëŒ€í•´ ê²€ìƒ‰ ìˆ˜í–‰
        for i, keyword in enumerate(keywords, 1):
            print(f"\nğŸ” [{i}/{len(keywords)}] ê²€ìƒ‰ í‚¤ì›Œë“œ: {keyword}")

            # Google ê²€ìƒ‰ ìˆ˜í–‰
            self.search_engine.search_google(keyword)

            # ê²€ìƒ‰ ê²°ê³¼ ë§í¬ë¥¼ ì§ì ‘ ë°©ë¬¸í•˜ë©° HTML ìˆ˜ì§‘
            visited_results = self.search_engine.visit_search_result_links(max_links=max_links_per_search)
            
            # ìƒˆë¡œìš´ URL í•„í„°ë§ ë° ë¶„ë¥˜
            if visited_results:
                print(f"  ğŸ“„ {len(visited_results)}ê°œì˜ ë§í¬ ë°©ë¬¸ ì™„ë£Œ")
                
                # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” URL ì œì™¸
                new_visited_results = [
                    (url, html) for url, html in visited_results 
                    if url not in existing_urls
                ]
                
                if new_visited_results:
                    print(f"  ğŸ†• {len(new_visited_results)}ê°œì˜ ìƒˆë¡œìš´ URL ë°œê²¬")
                    
                    # Gemini ë¶„ë¥˜ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶ˆë²• ì‚¬ì´íŠ¸ë§Œ í•„í„°ë§
                    filtered_urls, classification_results = self._classify_visited_results(new_visited_results)
                    
                    if filtered_urls:
                        # ë¶ˆë²• ì‚¬ì´íŠ¸ë¡œ íŒë³„ëœ URLë§Œ ì €ì¥
                        self.storage.save_results(filtered_urls, keyword, classification_results)
                        existing_urls.update(filtered_urls)
                        print(f"  ğŸ’¾ {len(filtered_urls)}ê°œì˜ ë¶ˆë²• ë„ë°• ì‚¬ì´íŠ¸ ì €ì¥ ì™„ë£Œ (í‚¤ì›Œë“œ: {keyword})")
                    else:
                        print(f"  â„¹ï¸ ë¶ˆë²• ë„ë°• ì‚¬ì´íŠ¸ ë¯¸ë°œê²¬ (í‚¤ì›Œë“œ: {keyword})")
                else:
                    print(f"  â„¹ï¸ ìƒˆë¡œìš´ URL ì—†ìŒ (í‚¤ì›Œë“œ: {keyword})")
            else:
                print(f"  âš ï¸ ë°©ë¬¸í•œ ë§í¬ ì—†ìŒ (í‚¤ì›Œë“œ: {keyword})")

            # ë‹¤ìŒ ê²€ìƒ‰ ì „ ëŒ€ê¸°
            if i < len(keywords):
                print(f"  â³ ë‹¤ìŒ ê²€ìƒ‰ê¹Œì§€ {delay}ì´ˆ ëŒ€ê¸° ì¤‘...")
                time.sleep(delay)

        # ë¸Œë¼ìš°ì € ì¢…ë£Œ ë° ê²°ê³¼ ì¶œë ¥
        self.search_engine.close()
        self._print_final_stats()

    def _print_final_stats(self):
        stats = self.storage.get_stats()
        print("\n" + "="*50)
        print("ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½")
        print("="*50)
        print(f"ğŸ“ ì´ í•­ëª© ìˆ˜: {stats['total_entries']}")
        print(f"ğŸ”— ê³ ìœ  URL ìˆ˜: {stats['unique_urls']}")
        print(f"ğŸ”¤ ì‚¬ìš©ëœ í‚¤ì›Œë“œ ìˆ˜: {stats['keywords_used']}")
        print(f"ğŸ’¾ ì¶œë ¥ íŒŒì¼: {self.settings.get('output_file', 'results.json')}")
        print("="*50)
